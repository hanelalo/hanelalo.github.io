---
title: CAP理论与分布式事务
date: 2022-03-15 22:17:20
tags: 分布式
categories: 分布式
cover: http://image.hanelalo.cn/image/202203152219479.jpg
---

> 最近做一些 interview 遇到了一些问题，有时间慢慢总结，慢慢学习。

# CAP 理论

现代的业务系统，在业务有了一定复杂度之后，都会逐渐向微服务、服务网格甚至无服务的方向演进。不管发展到那个阶段，都会脱离单体架构，演化成一个分布式的系统，分布式系统涉及共享数据的问题时，总会讨论一致性、可用性、分区容忍性。

* 一致性（Consistency）

  一致性表示任何时刻，任何分布式节点看到的数据都是符合预期的。

* 可用性（Availability）

  可用性主要用可靠性和可维护性两个指标衡量。可靠性使用平均无故障时间 (Mean Time Between Failure，MTBF) 来度量，可维护性使用平均修复时间 (Mean Time To Repair，MTTR) 来衡量，常说的系统可用性达到 5 个 9，其实就是说 `MTBF / (MTBF + MTTR) = 99.999%`。

* 分区容忍性（Partition Tolerance）

  指部分节点因网络问题相互失联后，形成了网络分区，依然能正常提供服务。

首先，分布式系统中，认为系统的网络通信是可靠的，但是事实上，因为网络环境的复杂性，并不能完全保证网络的可靠性，所以分区容忍性是必须保证的。

业界也一致认为，一致性和可用性不可兼得，所以现在的分布式系统，要么是 CP 系统，要么是 AP 系统。

* CP 系统

  牺牲可用性，当系统发生网络分区时，因为要保证数据的一致性，节点间的数据同步将不可预期地延长，或者直接响应失败，响应失败意味着可用性降低。

  Zookeeper 作为服务注册和发现中心时，当发生网络分区时，为了保证数据一致性，分区内重新进行了选举，这个过程是耗时的，选举过程中，整个服务注册与发现功能是不可用的，所以 Zookeeper 是一个 CP 系统。

* AP 系统

  牺牲一致性，当系统出现网络分区时，各个分区各自为政，继续对外服务，就可能出现数据不一致的情况，比如相同的请求分发到不同分区，导致得到的结果不一样。

  Eureka 同样也是服务注册与发现，和 Zookeeper 不同的是，每个服务往每个注册中心都进行注册，整个 Eureka 集群不分主从节点，只要还有一个节点活着，就能继续提供服务，只不过数据可能已经过时了，所以 Eureka 是一个 AP 系统。

# 分布式事务

## 概述

在微服务架构下，因为服务间需要做到数据自治，常常会进行分库，不同的微服务使用不同的库，当一个业务需要调用多个写服务时，就涉及到分布式事务的问题。

> 也有些非规范的做法，将写服务中访问数据库的代码打包成一个 jar 包，供其他服务依赖，其实所有的数据库写操作都在一个本地事务中，不存在分布式事务，这种的我认为不是微服务，因为代码还是产生了耦合，而微服务本身有一个目的就是解耦。

需要说明的是，一个本地事务执行过程，遵循事务的 ACID 原则，对于事务的隔离级别，又分成了**读未提交、读已提交、可重复度、串行执行。**

但到了分布式环境中，势必会有多个事务分布在不同的进程中，很难保证进程 A 中的事务和进程 B 中的事务会一起成功或一起失败，所以对于分布式事务的一致性分成了三类：

1. 弱一致性

   数据更新后，能容忍后续访问出现部分访问不到或者直接无法访问的情况。

2. 强一致性

   任意时刻，在任何一个节点看到的数据都是一致的。

3. 最终一致性

   允许数据在短时间内不一致，但经过一定时间最终达到一致。

而最终一致性起源于以 CAP 理论为基础扩展而来的 BASE 理论。

## BASE 理论

* Basically Available

  基本可用，当出现故障时，能容忍系统部分不可用，来保证核心功能可用。

* Soft State

  软状态，允许系统存在中间状态，且这种中间状态不会对系统造成影响。

* Eventually Consistent

  最终一致性，允许分布式系统中不同节点在同一时刻读到的状态不一致，但是随着时间推移，最终会保持一致。

## 解决方案

### 两阶段提交（TPC）/XA 协议

两阶段提交的实现中，有 2 个概念：

* 事务管理器

  协调各个本地资源管理器。

* 本地资源管理器

  一般是指执行各个本地事务的应用。

上面的两个管理器，一般都是数据库扮演的，而不需要应用的接入，事务管理器的选择也是数据库自行选择的。

两阶段提交的 2 步：

1. 准备阶段，事务管理器向所有参与事务的本地资源管理器询问是否准备就绪，资源管理器在响应这次询问时，会执行本地事务，但是不会提交，也就是说会一直持有数据的锁，保证对其他事务的隔离性。
2. 提交或回滚阶段，管理器收到所有资源管理器回复 Prepared 消息后，先将自己的本地事务 Commit，然后通知其他资源管理器进行 Commit，而如果收到一个资源管理器回复 Abort 或者资源管理器的回复超时了，就会先将自己的本地事务回滚，然后通知其他资源管理器回滚。

两阶段提交因为是所有本地事务一起提交，只是可能会有细微的网络延迟导致的时间差异而已，所以，使用两阶段提交需要有两个保证：

1. 网络是可靠的，不会丢失消息，比如丢失协调器发送给资源管理器的 Commit 的消息，如果这个消息丢了，是无法补救的。
2. 出现网络分区时可恢复，当资源管理器因为各种原因下线、停机后，最终能够恢复上线，而不是一直失联，因为在恢复后才能将未提交的本地事务提交。

两阶段提交虽然简单，但是缺点也比较显著：

* 单点问题

  在事务管理器和资源管理器的通信中，事务管理器对资源管理器超时这种情况还会有回滚的操作，但是如果挂掉的是事务管理器，资源管理器在接受事务管理器的指令这个点上是没有超时机制的，所以，如果事务管理器挂了，那么所有资源管理器会一直等待。

* 性能问题

  两阶段提交过程中，相当于将多个进程中的本地事务绑定成了一个整体，整个过程中有 2 次远程调用，3 次数据库写入（一阶段执行本地事务写 redo log，二阶段事务管理器持久化事务状态，二阶段资源管理器提交事务），整个过程将持续到最后一步中操作时间最长的本地事务完成为止。

* 一致性风险

  前文提到两阶段的必要网络条件和网络分区可恢复保证，但是实际上，并不能 100% 保证不会因为网络故障导致丢失 Commit 消息，也不能保证资源管理器下线之后一定会上线，这个时候就会出现一致性问题，有些本地事务提交了，有些本地事务一直在等待或者直接丢失。

### 三阶段提交

为了解决上述两阶段提交的部分问题，又演化出了三阶段提交。具体来讲，其实解决的是单点问题和准备阶段的性能问题。

三阶段提交将两阶段提交中的准备阶段又细分为 CanCommit、Pre Commit 两个阶段，二阶段中的提交或回滚阶段叫做 Do Commit。

* Can Commit

  询问每个资源管理器，根据当前的自身状态评估该事务是否有可能顺利完成。

* Pre Commit

  每个资源管理器执行本地事务，但是不提交，其实就是写了 redo log。

* Do Commit

  和二阶段一样，提交或者回滚。

第一阶段并不会执行事务中的一些 SQL，所以会很快，这一步的存在是为了更好的保证事务能顺利完成，向二阶段提交那样，其中一个资源管理器失败了，而其他分支事务都成功了，最后依然还是回滚，那么那写成功的分支其实是做了无用功，所以这在一定程度上也缓解了两阶段提交的第一阶段的性能问题。

第二阶段依然是执行事务内的 SQL，写入 redo log，但事务不会提交。

第三阶段，和两阶段提交相似，全部成功就提交，其中一个失败就是回滚，不同的是，对于事务管理器在第三阶段指令超时的情况，这里不会一直等待，资源管理器超时后会执行提交操作。

性能上，因为多了一阶段的检查，所以如果是整个分布式事务最终能够成功提交的场景下，三阶段提交性能可能甚至不如两阶段提交，而对于回滚的场景，因为做了提前的检查，所以出现大部分资源管理器都写了 redo log 之后依然回滚的概率比较小，更大概率是在一阶段就直接失败了，所以，对于回滚的场景，三阶段提交相对于两阶段提交有所提升。

一致性方面，三阶段提交可能会导致数据错误，比如在第三阶段资源管理器应该受到 Abort 消息而回滚，但是网络原因导致超时，最后实际执行的是 Commit，这个时候就会出现数据不一致的情况。

### 可靠事件队列

现在加入有 A、B、C 三个服务要一起完成一个业务，分布式事务一开始由 A 发起。

1.  A 服务完成本地事务，并在自己的数据库建立一张消息表，其中主要包含事务 ID、事务的内容、事务状态（进行中），这个，这两个操作是在 A 服务的同一个本地事务中完成，以免出现事务执行成功，但是消息存储失败的情况。
2. 在系统中建立消息服务，定时轮询第 1 步中的消息表，如果由新的消息，就发送到 B、C 服务，此时会有以下 4 中情况：
   1. B、C 都接收到新的消息，并成功执行自己的本地事务并提交，然后将结果返回给 A 服务，A服务将消息表中指定的消息事务状态改为已完成，整个分布式事务结束。
   2. B、C 两个服务中，至少一个没有成功接收到消息，导致整个分布式事务的状态一直是进行中，此时消息服务器会继续发送消息，知道 B、C 全都成功消费，因为就会出现重复消息的问题，所以 B、C 服务需要做幂等处理，可以考虑将分布式事务的事务 ID 作为标识来进行幂等处理，如果某个 ID 已经处理过了，就不再进行执行本地事务逻辑。
   3. B、C 至少有一个消费事件失败，导致分布式事务不能完成，这种情况，失败的服务依然会一直消费，知道人工介入，这说明可靠事件队列这种方式是只许成功，不许失败的。
   4. 当 B、C 都消费成功，但是最后发送的返回结果因为网络问题丢失了，此时 A 服务过一段事件会重新发送消息，B、C 会重新消费，但是因为已经做了幂等处理，所以不会再执行事务逻辑，而只是再次触发发送执行结果的逻辑而已。

> 这种方式其实可以通过支持事务消息的消息中间件来实现，比如 RocketMQ。

### TCC 事务

TCC 是 Try-Confirm-Cancel 的缩写，

* Try

  尝试执行阶段，完成事务可执行性的检查，预留好执行事务需要的资源，比如锁定库存。

* Confirm

  确认执行阶段，如果 Try 阶段执行成功，将进入确认执行阶段，这一阶段不执行业务检查，使用 Try 阶段预留的资源直接完成业务逻辑，任何一方出现问题都会重复执行，知道成功为止，所以这一步必须保证幂等性。

* Cancel

  取消执行阶段，如果 Try 阶段任何一方出现任何问题，将会进入取消执行阶段，取消执行阶段会释放 Try 阶段的资源，如果在这个过程中任何一方出现问题，会一直重复执行，知道释放资源成功，所以这一步必须保证幂等性。

会发现 TCC 事务和两阶段提交很像，但是，两阶段提交是数据库之间自行完成的，不需要应用代码接入，但是 TCC 事务是需要应用层面的代码开发来支持 3 个阶段的操作的，对业务代码侵入性比较强。

### SAGA 事务

>  SAGA 事务我第一次是在《微服务架构设计模式》这本书上见过。

到目前位置介绍的方案，基本都保证了事务的隔离性，以防止出现“超售”的情况，但是前面几种方案都是建立在这多个分支事务都是可控的情况下，而如果是要对接第三方系统时，前面的方案就没用了，比如在接入微信支付或者支付宝支付时，这写平台肯定不会专门针对每一个平台提供二、三阶段提交或者 TCC 的支持，这个时候，就出现了  SAGA 事务这种解决方案。

SAGA 要求为每一个事务提供一个补偿事务来保证最终一致性。

比如，分布式事务 T 分成 T1、T2、T3...Tn 一共 n 个分支事务，那么就必须提供 C1、C2、C3...Cn 一共 n 个补偿事务。

如果 T1 到 Tn 都执行成功了，那么整个分布式事务就成功了，而如果在执行 Ti 时失败了，那么 T1 到 Ti 都需要回滚，此时就需要执行 Ci 到 C1 的补偿事务来保证最终一致性。

需要注意的是，SAGA 事务在全局上的隔离级别是读未提交，而在局部分支上是根据实际的数据库隔离级别来决定的。

总是，对于分布式事务的解决方案，并没有所谓的银弹，只有因地制宜而已，对于这些解决方案，由开源 Seata 框架支持了上述几乎所有方案，并且在两阶段提交的基础上做改进，提出了 AT 模式。

> Seata：https://seata.io/
>
> 另外，《深入理解 Java 虚拟机》的作者周志明老师开源了新书[《凤凰架构》](http://icyfenix.cn/)中也有专门对分布式事务做讲解。

